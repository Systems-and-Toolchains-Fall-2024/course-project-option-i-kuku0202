{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/Cellar/apache-spark/3.5.2/libexec/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/junyixu/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/junyixu/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-76ff2bb6-f170-4523-8bf0-425f72dfb73a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      ":: resolution report :: resolve 109ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-76ff2bb6-f170-4523-8bf0-425f72dfb73a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "24/11/10 15:14:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/10 15:14:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/10 15:14:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "/usr/local/opt/apache-spark/libexec/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "properties = {\n",
    "    'username': 'postgres',\n",
    "    'password': 'Fdl77n5h402s/XJY',\n",
    "    'url': \"jdbc:postgresql://localhost:5432/postgres\",\n",
    "    'table': 'fifa.clean_data',\n",
    "    'driver': 'org.postgresql.Driver'\n",
    "}\n",
    "\n",
    "def write_to_pgadmin(df, mode='overwrite'):\n",
    "    df.write.format('jdbc').mode(mode)\\\n",
    "        .option(\"url\", properties['url'])\\\n",
    "        .option(\"dbtable\", properties['table'])\\\n",
    "        .option(\"user\", properties['username'])\\\n",
    "        .option(\"password\", properties['password'])\\\n",
    "        .option(\"Driver\", properties['driver'])\\\n",
    "        .save()\n",
    "\n",
    "def read_from_pgadmin():\n",
    "    return spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", properties['url'])\\\n",
    "        .option(\"dbtable\", properties['table'])\\\n",
    "        .option(\"user\", properties['username'])\\\n",
    "        .option(\"password\", properties['password'])\\\n",
    "        .option(\"Driver\", properties['driver'])\\\n",
    "        .load()\n",
    "\n",
    "appName = \"Big Data Analytics\"\n",
    "master = \"local\"\n",
    "\n",
    "conf = pyspark.SparkConf().\\\n",
    "    set('spark.jars.packages', 'org.postgresql:postgresql:42.7.0')\\\n",
    "    .setAppName(appName).setMaster(master)\n",
    "\n",
    "# Create Spark Context with the new configurations rather than relying on the default one\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# You need to create SQL Context to conduct some database operations like what we will see later.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/10 15:15:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from preprocess import *\n",
    "df = read_from_pgadmin()\n",
    "preprocess_pipeline = get_preprocess_pipeline()\n",
    "preprocess_pipeline_model = preprocess_pipeline.fit(df)\n",
    "df_processed = preprocess_pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|outcome|            features|\n",
      "+-------+--------------------+\n",
      "|   94.0|[6.00629302994062...|\n",
      "|   93.0|[6.43531396065066...|\n",
      "|   90.0|[6.64982442600569...|\n",
      "|   90.0|[6.22080349529564...|\n",
      "|   90.0|[6.00629302994062...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_processed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df_processed.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 264:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test RMSE: 2.5087356589009286\n",
      "Best Model Parameters: regParam=0.1, elasticNetParam=0.0, maxIter=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='outcome')\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"outcome\", predictionCol=\"prediction\")\n",
    "\n",
    "# Set up parameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "cross_validator = CrossValidator(estimator=lr,\n",
    "                                 estimatorParamMaps=param_grid,\n",
    "                                 evaluator=evaluator,\n",
    "                                 numFolds=5)\n",
    "\n",
    "lr_cv_model = cross_validator.fit(train_df)\n",
    "\n",
    "# Get the best model\n",
    "lr_best_model = lr_cv_model.bestModel\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "predictions = lr_best_model.transform(test_df)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Best Model Test RMSE: {rmse}\")\n",
    "print(f\"Best Model Parameters: regParam={lr_best_model._java_obj.getRegParam()}, elasticNetParam={lr_best_model._java_obj.getElasticNetParam()}, maxIter={lr_best_model._java_obj.getMaxIter()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12994:>                                                      (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test RMSE: 1.6315286679270953\n",
      "Best Model Parameters: maxDepth=5, maxIter=20, stepSize=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"outcome\")\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"outcome\", predictionCol=\"prediction\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(gbt.maxIter, [10, 20, 50]) \\\n",
    "    .addGrid(gbt.stepSize, [0.1, 0.2, 0.3]) \\\n",
    "    .build()\n",
    "\n",
    "cross_validator = CrossValidator(estimator=gbt,\n",
    "                                 estimatorParamMaps=param_grid,\n",
    "                                 evaluator=evaluator,\n",
    "                                 numFolds=5)\n",
    "\n",
    "gbt_cv_model = cross_validator.fit(train_df)\n",
    "\n",
    "# Get the best model\n",
    "gbt_best_model = gbt_cv_model.bestModel\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "predictions = gbt_best_model.transform(test_df)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Best Model Test RMSE: {rmse}\")\n",
    "print(f\"Best Model Parameters: maxDepth={gbt_best_model._java_obj.getMaxDepth()}, maxIter={gbt_best_model._java_obj.getMaxIter()}, stepSize={gbt_best_model._java_obj.getStepSize()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "18763",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
