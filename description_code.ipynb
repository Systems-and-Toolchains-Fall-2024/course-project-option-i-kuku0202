{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initialize SparkSQL Application - Create SQL Context</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/Cellar/apache-spark/3.5.2/libexec/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/junyixu/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/junyixu/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-58745b14-8326-4256-b651-c4db1c806d27;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      ":: resolution report :: resolve 186ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-58745b14-8326-4256-b651-c4db1c806d27\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/9ms)\n",
      "24/10/09 21:08:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/usr/local/opt/apache-spark/libexec/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following lines if you are using Windows!\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "properties = {\n",
    "    'username': 'postgres',\n",
    "    'password': 'Fdl77n5h402s/XJY',\n",
    "    'url': \"jdbc:postgresql://localhost:5432/postgres\",\n",
    "    'table': 'fifa.player_data',\n",
    "    'driver': 'org.postgresql.Driver'\n",
    "}\n",
    "\n",
    "def write_to_pgadmin(df, mode='overwrite'):\n",
    "    df.write.format('jdbc').mode(mode)\\\n",
    "        .option(\"url\", properties['url'])\\\n",
    "        .option(\"dbtable\", properties['table'])\\\n",
    "        .option(\"user\", properties['username'])\\\n",
    "        .option(\"password\", properties['password'])\\\n",
    "        .option(\"Driver\", properties['driver'])\\\n",
    "        .save()\n",
    "\n",
    "def read_from_pgadmin():\n",
    "    return spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", properties['url'])\\\n",
    "        .option(\"dbtable\", properties['table'])\\\n",
    "        .option(\"user\", properties['username'])\\\n",
    "        .option(\"password\", properties['password'])\\\n",
    "        .option(\"Driver\", properties['driver'])\\\n",
    "        .load()\n",
    "\n",
    "appName = \"Big Data Analytics\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "# conf = pyspark.SparkConf()\\\n",
    "#     .set('spark.driver.host','127.0.0.1')\\\n",
    "#     .setAppName(appName)\\\n",
    "#     .setMaster(master)\n",
    "\n",
    "conf = pyspark.SparkConf().\\\n",
    "    set('spark.jars.packages', 'org.postgresql:postgresql:42.7.0')\\\n",
    "    .setAppName(appName).setMaster(master)\n",
    "\n",
    "# Create Spark Context with the new configurations rather than relying on the default one\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# You need to create SQL Context to conduct some database operations like what we will see later.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-I: Build and populate necessary tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset from .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/09 21:08:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------------+--------------------+----------------+-------+---------+---------+--------+---+----------+---------+---------+------------+-------------------+--------------------+------------+-------------+------------------+----------------+-----------+-------------------------+--------------+----------------+--------------+---------------+--------------------+--------------+---------+-----------+------------------------+-------------+----------------+---------+------------------+--------------------+--------------------+----+--------+-------+---------+---------+------+------------------+-------------------+--------------------------+-----------------------+-----------------+---------------+-----------+-----------------+------------------+------------------+---------------------+---------------------+----------------+------------------+----------------+----------------+-------------+-------------+--------------+----------------+--------------------+-----------------------+---------------------+----------------+-------------------+-------------------+---------------------------+-------------------------+------------------------+------------------+--------------------+-------------------+-----------------------+--------------------+-----------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+--------------------+--------------------+--------------------+--------------------+--------------------+----+------+---------+\n",
      "|sofifa_id|          player_url|       short_name|           long_name|player_positions|overall|potential|value_eur|wage_eur|age|       dob|height_cm|weight_kg|club_team_id|          club_name|         league_name|league_level|club_position|club_jersey_number|club_loaned_from|club_joined|club_contract_valid_until|nationality_id|nationality_name|nation_team_id|nation_position|nation_jersey_number|preferred_foot|weak_foot|skill_moves|international_reputation|    work_rate|       body_type|real_face|release_clause_eur|         player_tags|       player_traits|pace|shooting|passing|dribbling|defending|physic|attacking_crossing|attacking_finishing|attacking_heading_accuracy|attacking_short_passing|attacking_volleys|skill_dribbling|skill_curve|skill_fk_accuracy|skill_long_passing|skill_ball_control|movement_acceleration|movement_sprint_speed|movement_agility|movement_reactions|movement_balance|power_shot_power|power_jumping|power_stamina|power_strength|power_long_shots|mentality_aggression|mentality_interceptions|mentality_positioning|mentality_vision|mentality_penalties|mentality_composure|defending_marking_awareness|defending_standing_tackle|defending_sliding_tackle|goalkeeping_diving|goalkeeping_handling|goalkeeping_kicking|goalkeeping_positioning|goalkeeping_reflexes|goalkeeping_speed|  ls|  st|  rs|  lw|  lf|  cf|  rf|  rw| lam| cam| ram|  lm| lcm|  cm| rcm|  rm| lwb| ldm| cdm| rdm| rwb|  lb| lcb|  cb| rcb|  rb|  gk|     player_face_url|       club_logo_url|       club_flag_url|     nation_logo_url|     nation_flag_url|year|gender|unique_id|\n",
      "+---------+--------------------+-----------------+--------------------+----------------+-------+---------+---------+--------+---+----------+---------+---------+------------+-------------------+--------------------+------------+-------------+------------------+----------------+-----------+-------------------------+--------------+----------------+--------------+---------------+--------------------+--------------+---------+-----------+------------------------+-------------+----------------+---------+------------------+--------------------+--------------------+----+--------+-------+---------+---------+------+------------------+-------------------+--------------------------+-----------------------+-----------------+---------------+-----------+-----------------+------------------+------------------+---------------------+---------------------+----------------+------------------+----------------+----------------+-------------+-------------+--------------+----------------+--------------------+-----------------------+---------------------+----------------+-------------------+-------------------+---------------------------+-------------------------+------------------------+------------------+--------------------+-------------------+-----------------------+--------------------+-----------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+--------------------+--------------------+--------------------+--------------------+--------------------+----+------+---------+\n",
      "|   158023|https://sofifa.co...|         L. Messi|Lionel Andrés Mes...|              CF|     93|       95|  1.005E8|550000.0| 27|1987-06-24|      169|       67|       241.0|       FC Barcelona|Spain Primera Div...|           1|           CF|                10|            NULL| 2004-07-01|                     2018|            52|       Argentina|        1369.0|             CF|                  10|          Left|        3|          4|                       5|   Medium/Low|   Normal (170-)|      Yes|              NULL|#Speedster, #Drib...|Finesse Shot, Spe...|  93|      89|     86|       96|       27|    63|                84|                 94|                        71|                     89|               85|             96|         89|               90|                76|                96|                   96|                   90|              94|                94|              95|              80|           73|           77|            60|              88|                  48|                     22|                   92|              90|                 76|               NULL|                         25|                       21|                      20|                 6|                  11|                 15|                     14|                   8|             NULL|89+3|89+3|89+3|92+3|90+3|90+3|90+3|92+3|92+3|92+3|92+3|90+3|79+3|79+3|79+3|90+3|62+3|62+3|62+3|62+3|62+3|54+3|45+3|45+3|45+3|54+3|15+3|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|2015|  male|        0|\n",
      "|    20801|https://sofifa.co...|Cristiano Ronaldo|Cristiano Ronaldo...|          LW, LM|     92|       92|    7.9E7|375000.0| 29|1985-02-05|      185|       80|       243.0|     Real Madrid CF|Spain Primera Div...|           1|           LW|                 7|            NULL| 2009-07-01|                     2018|            38|        Portugal|        1354.0|             LW|                   7|         Right|        4|          5|                       5|     High/Low|   Normal (185+)|      Yes|              NULL|#Speedster, #Drib...|Power Free-Kick, ...|  93|      93|     81|       91|       32|    79|                83|                 95|                        86|                     82|               87|             93|         88|               79|                72|                92|                   91|                   94|              93|                90|              63|              94|           94|           89|            79|              93|                  63|                     24|                   91|              81|                 85|               NULL|                         22|                       31|                      23|                 7|                  11|                 15|                     14|                  11|             NULL|91+1|91+1|91+1|89+3|91+1|91+1|91+1|89+3|89+3|89+3|89+3|87+3|77+3|77+3|77+3|87+3|63+3|63+3|63+3|63+3|63+3|57+3|52+3|52+3|52+3|57+3|16+3|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|2015|  male|        1|\n",
      "|     9014|https://sofifa.co...|        A. Robben|        Arjen Robben|      RM, LM, RW|     90|       90|   5.45E7|275000.0| 30|1984-01-23|      180|       80|        21.0|  FC Bayern München|German 1. Bundesliga|           1|          SUB|                10|            NULL| 2009-08-28|                     2017|            34|     Netherlands|      105035.0|             RS|                  11|          Left|        2|          4|                       5|     High/Low|Normal (170-185)|      Yes|              NULL|#Speedster, #Drib...|Diver, Injury Pro...|  93|      86|     83|       92|       32|    64|                80|                 85|                        50|                     86|               86|             93|         85|               83|                76|                90|                   93|                   93|              93|                89|              91|              86|           61|           78|            65|              90|                  47|                     39|                   89|              84|                 80|               NULL|                         29|                       26|                      26|                10|                   8|                 11|                      5|                  15|             NULL|84+3|84+3|84+3|88+2|87+3|87+3|87+3|88+2|88+2|88+2|88+2|87+3|78+3|78+3|78+3|87+3|64+3|64+3|64+3|64+3|64+3|55+3|46+3|46+3|46+3|55+3|14+3|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|2015|  male|        2|\n",
      "|    41236|https://sofifa.co...|   Z. Ibrahimović|  Zlatan Ibrahimović|              ST|     90|       90|   5.25E7|275000.0| 32|1981-10-03|      195|       95|        73.0|Paris Saint-Germain|      French Ligue 1|           1|           ST|                10|            NULL| 2012-07-01|                     2016|            46|          Sweden|        1363.0|             ST|                  10|         Right|        4|          4|                       5|   Medium/Low|   Normal (185+)|      Yes|              NULL|#Poacher, #Aerial...|Power Free-Kick, ...|  76|      91|     81|       86|       34|    86|                76|                 91|                        76|                     84|               92|             88|         80|               80|                76|                90|                   74|                   77|              86|                85|              41|              93|           72|           78|            93|              88|                  84|                     20|                   86|              83|                 91|               NULL|                         25|                       41|                      27|                13|                  15|                 10|                      9|                  12|             NULL|87+3|87+3|87+3|84+3|86+3|86+3|86+3|84+3|86+3|86+3|86+3|83+3|76+3|76+3|76+3|83+3|61+3|65+3|65+3|65+3|61+3|56+3|55+3|55+3|55+3|56+3|17+3|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|2015|  male|        3|\n",
      "|   167495|https://sofifa.co...|         M. Neuer|  Manuel Peter Neuer|              GK|     90|       90|   6.35E7|300000.0| 28|1986-03-27|      193|       92|        21.0|  FC Bayern München|German 1. Bundesliga|           1|           GK|                 1|            NULL| 2011-07-01|                     2019|            21|         Germany|        1337.0|             GK|                   1|         Right|        4|          1|                       5|Medium/Medium|   Normal (185+)|      Yes|              NULL|                NULL|GK Up for Corners...|NULL|    NULL|   NULL|     NULL|     NULL|  NULL|                25|                 25|                        25|                     42|               25|             25|         25|               25|                41|                31|                   58|                   61|              43|                89|              35|              42|           78|           44|            83|              25|                  29|                     30|                   25|              20|                 37|               NULL|                         25|                       25|                      25|                87|                  85|                 92|                     90|                  86|               60|38+3|38+3|38+3|36+3|37+3|37+3|37+3|36+3|36+3|36+3|36+3|38+3|36+3|36+3|36+3|38+3|36+3|40+3|40+3|40+3|36+3|36+3|38+3|38+3|38+3|36+3|87+3|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|https://cdn.sofif...|2015|  male|        4|\n",
      "+---------+--------------------+-----------------+--------------------+----------------+-------+---------+---------+--------+---+----------+---------+---------+------------+-------------------+--------------------+------------+-------------+------------------+----------------+-----------+-------------------------+--------------+----------------+--------------+---------------+--------------------+--------------+---------+-----------+------------------------+-------------+----------------+---------+------------------+--------------------+--------------------+----+--------+-------+---------+---------+------+------------------+-------------------+--------------------------+-----------------------+-----------------+---------------+-----------+-----------------+------------------+------------------+---------------------+---------------------+----------------+------------------+----------------+----------------+-------------+-------------+--------------+----------------+--------------------+-----------------------+---------------------+----------------+-------------------+-------------------+---------------------------+-------------------------+------------------------+------------------+--------------------+-------------------+-----------------------+--------------------+-----------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+--------------------+--------------------+--------------------+--------------------+--------------------+----+------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- sofifa_id: integer (nullable = true)\n",
      " |-- player_url: string (nullable = true)\n",
      " |-- short_name: string (nullable = true)\n",
      " |-- long_name: string (nullable = true)\n",
      " |-- player_positions: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- potential: integer (nullable = true)\n",
      " |-- value_eur: double (nullable = true)\n",
      " |-- wage_eur: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- height_cm: integer (nullable = true)\n",
      " |-- weight_kg: integer (nullable = true)\n",
      " |-- club_team_id: double (nullable = true)\n",
      " |-- club_name: string (nullable = true)\n",
      " |-- league_name: string (nullable = true)\n",
      " |-- league_level: integer (nullable = true)\n",
      " |-- club_position: string (nullable = true)\n",
      " |-- club_jersey_number: integer (nullable = true)\n",
      " |-- club_loaned_from: string (nullable = true)\n",
      " |-- club_joined: date (nullable = true)\n",
      " |-- club_contract_valid_until: integer (nullable = true)\n",
      " |-- nationality_id: integer (nullable = true)\n",
      " |-- nationality_name: string (nullable = true)\n",
      " |-- nation_team_id: double (nullable = true)\n",
      " |-- nation_position: string (nullable = true)\n",
      " |-- nation_jersey_number: integer (nullable = true)\n",
      " |-- preferred_foot: string (nullable = true)\n",
      " |-- weak_foot: integer (nullable = true)\n",
      " |-- skill_moves: integer (nullable = true)\n",
      " |-- international_reputation: integer (nullable = true)\n",
      " |-- work_rate: string (nullable = true)\n",
      " |-- body_type: string (nullable = true)\n",
      " |-- real_face: string (nullable = true)\n",
      " |-- release_clause_eur: string (nullable = true)\n",
      " |-- player_tags: string (nullable = true)\n",
      " |-- player_traits: string (nullable = true)\n",
      " |-- pace: integer (nullable = true)\n",
      " |-- shooting: integer (nullable = true)\n",
      " |-- passing: integer (nullable = true)\n",
      " |-- dribbling: integer (nullable = true)\n",
      " |-- defending: integer (nullable = true)\n",
      " |-- physic: integer (nullable = true)\n",
      " |-- attacking_crossing: integer (nullable = true)\n",
      " |-- attacking_finishing: integer (nullable = true)\n",
      " |-- attacking_heading_accuracy: integer (nullable = true)\n",
      " |-- attacking_short_passing: integer (nullable = true)\n",
      " |-- attacking_volleys: integer (nullable = true)\n",
      " |-- skill_dribbling: integer (nullable = true)\n",
      " |-- skill_curve: integer (nullable = true)\n",
      " |-- skill_fk_accuracy: integer (nullable = true)\n",
      " |-- skill_long_passing: integer (nullable = true)\n",
      " |-- skill_ball_control: integer (nullable = true)\n",
      " |-- movement_acceleration: integer (nullable = true)\n",
      " |-- movement_sprint_speed: integer (nullable = true)\n",
      " |-- movement_agility: integer (nullable = true)\n",
      " |-- movement_reactions: integer (nullable = true)\n",
      " |-- movement_balance: integer (nullable = true)\n",
      " |-- power_shot_power: integer (nullable = true)\n",
      " |-- power_jumping: integer (nullable = true)\n",
      " |-- power_stamina: integer (nullable = true)\n",
      " |-- power_strength: integer (nullable = true)\n",
      " |-- power_long_shots: integer (nullable = true)\n",
      " |-- mentality_aggression: integer (nullable = true)\n",
      " |-- mentality_interceptions: integer (nullable = true)\n",
      " |-- mentality_positioning: integer (nullable = true)\n",
      " |-- mentality_vision: integer (nullable = true)\n",
      " |-- mentality_penalties: integer (nullable = true)\n",
      " |-- mentality_composure: string (nullable = true)\n",
      " |-- defending_marking_awareness: integer (nullable = true)\n",
      " |-- defending_standing_tackle: integer (nullable = true)\n",
      " |-- defending_sliding_tackle: integer (nullable = true)\n",
      " |-- goalkeeping_diving: integer (nullable = true)\n",
      " |-- goalkeeping_handling: integer (nullable = true)\n",
      " |-- goalkeeping_kicking: integer (nullable = true)\n",
      " |-- goalkeeping_positioning: integer (nullable = true)\n",
      " |-- goalkeeping_reflexes: integer (nullable = true)\n",
      " |-- goalkeeping_speed: integer (nullable = true)\n",
      " |-- ls: string (nullable = true)\n",
      " |-- st: string (nullable = true)\n",
      " |-- rs: string (nullable = true)\n",
      " |-- lw: string (nullable = true)\n",
      " |-- lf: string (nullable = true)\n",
      " |-- cf: string (nullable = true)\n",
      " |-- rf: string (nullable = true)\n",
      " |-- rw: string (nullable = true)\n",
      " |-- lam: string (nullable = true)\n",
      " |-- cam: string (nullable = true)\n",
      " |-- ram: string (nullable = true)\n",
      " |-- lm: string (nullable = true)\n",
      " |-- lcm: string (nullable = true)\n",
      " |-- cm: string (nullable = true)\n",
      " |-- rcm: string (nullable = true)\n",
      " |-- rm: string (nullable = true)\n",
      " |-- lwb: string (nullable = true)\n",
      " |-- ldm: string (nullable = true)\n",
      " |-- cdm: string (nullable = true)\n",
      " |-- rdm: string (nullable = true)\n",
      " |-- rwb: string (nullable = true)\n",
      " |-- lb: string (nullable = true)\n",
      " |-- lcb: string (nullable = true)\n",
      " |-- cb: string (nullable = true)\n",
      " |-- rcb: string (nullable = true)\n",
      " |-- rb: string (nullable = true)\n",
      " |-- gk: string (nullable = true)\n",
      " |-- player_face_url: string (nullable = true)\n",
      " |-- club_logo_url: string (nullable = true)\n",
      " |-- club_flag_url: string (nullable = true)\n",
      " |-- nation_logo_url: string (nullable = true)\n",
      " |-- nation_flag_url: string (nullable = true)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- unique_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit, monotonically_increasing_id\n",
    "root = 'data'\n",
    "soccer_data = spark.read.csv(os.path.join(root, 'players_15.csv'), header=True, inferSchema=True)\n",
    "schema = soccer_data.schema\n",
    "soccer_data = soccer_data.withColumn('year', lit(2015))\n",
    "soccer_data = soccer_data.withColumn('gender', lit('male'))\n",
    "for file in sorted(os.listdir(root)):\n",
    "    if file.split('_')[-1][:-4] != '15':\n",
    "        df = spark.read.csv(os.path.join(root, file), header=True, schema=schema)\n",
    "        df = df.withColumn('year', lit(2000+int(file.split('_')[-1][:-4])))      # add new column for the year\n",
    "        if file.startswith('player'):\n",
    "            df = df.withColumn('gender', lit('Male'))\n",
    "        else:\n",
    "            df = df.withColumn('gender', lit('Female'))\n",
    "        soccer_data = soccer_data.union(df)\n",
    "\n",
    "soccer_data = soccer_data.withColumn('unique_id', monotonically_increasing_id())    # add unique id to each row\n",
    "soccer_data.show(5)\n",
    "soccer_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push data to Postgres Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o295.save.\n: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:443)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:476)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:218)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:200)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:394)\n\tat java.net.Socket.connect(Socket.java:606)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)\n\t... 50 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwrite_to_pgadmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoccer_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mwrite_to_pgadmin\u001b[0;34m(df, mode)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_to_pgadmin\u001b[39m(df, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     21\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musername\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o295.save.\n: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:443)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:476)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:218)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:200)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:394)\n\tat java.net.Socket.connect(Socket.java:606)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)\n\t... 50 more\n"
     ]
    }
   ],
   "source": [
    "write_to_pgadmin(soccer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-II: Conduct analytics on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Year X, what were the Y clubs that had the highest number of players with \n",
    "contracts ending in year Z (or after)? \n",
    "- X is a year between (2015 and 2022, inclusively). \n",
    "- Y is a positive integer. \n",
    "- Z is a year that can hold the value of 2023 or a year after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_club_based_on_contract_ending(df, X, Y, Z):\n",
    "    assert type(X)==int and X >= 2015 and X <= 2022, 'X should be a year between (2014 and 2022, inclusively)'\n",
    "    assert type(Y)==int and Y > 0, 'Y should be a positive integer'\n",
    "    assert type(Z)==int and Z >= 2023, 'Z should be 2023 or a year after it'\n",
    "    df_valid = df[(df['year']==X) & (df['club_contract_valid_until']==Z)]\n",
    "    df_num_valid_per_club = df_valid.groupby('club_name').count().orderBy('count', ascending=False)\n",
    "    top_Y_club = df_num_valid_per_club.select('club_name').limit(Y).rdd.flatMap(lambda x: x).collect()\n",
    "    return top_Y_club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Real Madrid CF', 'Real Sociedad', 'RCD Espanyol de Barcelona']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = read_from_pgadmin()\n",
    "df = df[df['gender']=='Male']\n",
    "X = 2020\n",
    "Y = 3\n",
    "Z = 2025\n",
    "print(get_club_based_on_contract_ending(df, X, Y, Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the X clubs with the highest (or lowest) average player age for a given \n",
    "year Y.  \n",
    "- X represents a positive integer, but you should handle a scenario if \n",
    "X is not positive value. \n",
    "- Y represents a year between 2015 and 2022 inclusively.  \n",
    "- Provide the user with the ability to choose if they want the highest \n",
    "average age or the lowest average age.  \n",
    "- Make sure to handle this scenario as well: if the user requests 5 \n",
    "clubs with highest averages but there are 3 clubs that share the \n",
    "same count at rank number 5, please include all of them in your \n",
    "output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_club_based_on_age(df, X, Y, mode):\n",
    "    assert mode=='highest' or mode=='lowest', 'invalid mode'\n",
    "    assert type(X)==int and X > 0, 'X shoul be a positive integer'\n",
    "    assert type(Y)==int and Y >= 2015 and Y <= 2022, 'Y should be a year between 2014 and 2022 inclusively'\n",
    "    df_valid = df[df['year']==Y]\n",
    "    df_avg_age_per_club = df_valid.select(['club_name', 'age']).groupby('club_name').mean()\n",
    "    df_unique_age = df_avg_age_per_club.select('avg(age)').distinct()\n",
    "    if mode == 'highest':\n",
    "        largest_Xth_age = df_unique_age.orderBy('avg(age)', ascending=False).collect()[X-1]['avg(age)']\n",
    "        return df_avg_age_per_club[df_avg_age_per_club['avg(age)'] >= largest_Xth_age].select('club_name').rdd.flatMap(lambda x:x).collect()\n",
    "    else:\n",
    "        smallest_Xth_age = df_unique_age.orderBy('avg(age)').collect()[X-1]['avg(age)']\n",
    "        return df_avg_age_per_club[df_avg_age_per_club['avg(age)'] <= smallest_Xth_age].select('club_name').rdd.flatMap(lambda x:x).collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Club Athletico Paranaense', 'Fortaleza', 'Santos', 'Avaí FC', 'Botafogo', 'Goiás', 'Internacional', 'Fluminense', 'Cruzeiro', 'Bahia', 'Grêmio', 'Clube Atlético Mineiro', 'Ceará Sporting Club', 'Associação Chapecoense de Futebol']\n"
     ]
    }
   ],
   "source": [
    "df = read_from_pgadmin()\n",
    "df = df[df['gender']=='Male']\n",
    "X = 7\n",
    "Y = 2020\n",
    "print(get_club_based_on_age(df, X, Y, mode='highest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most popular nationality in the dataset for each year? (i.e. display the \n",
    "most frequent nation for 2015, 2016, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_popular_nationality_per_year(df):\n",
    "    unique_years = df.select('year').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "    for year in sorted(unique_years):\n",
    "        df_nationality_count = df[df['year']==year].groupby('nationality_name').count()\n",
    "        most_popular_nationality = df_nationality_count.orderBy('count', ascending=False).collect()[0]['nationality_name']\n",
    "        print(f'The most popular nationality in year {year} is {most_popular_nationality}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most popular nationality in year 2015 is England\n",
      "The most popular nationality in year 2016 is England\n",
      "The most popular nationality in year 2017 is England\n",
      "The most popular nationality in year 2018 is England\n",
      "The most popular nationality in year 2019 is England\n",
      "The most popular nationality in year 2020 is England\n",
      "The most popular nationality in year 2021 is England\n",
      "The most popular nationality in year 2022 is England\n"
     ]
    }
   ],
   "source": [
    "df = read_from_pgadmin()\n",
    "df[df['gender']=='Male']\n",
    "get_most_popular_nationality_per_year(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "18763",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
